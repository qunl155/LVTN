# ğŸ“Š THUYáº¾T TRÃŒNH Báº¢O Vá»† LUáº¬N VÄ‚N (10 SLIDES)

## Äá»€ TÃ€I: Há»† THá»NG PHÃ‚N TÃCH Cáº¢M XÃšC Sá»¬ Dá»¤NG PHOBERT

---

## ğŸ“‹ Cáº¤U TRÃšC (10 SLIDES - ~15 PHÃšT)

| Slide | Ná»™i dung |
|:-----:|:---------|
| 1 | Trang bÃ¬a |
| 2 | Tá»•ng quan & Má»¥c tiÃªu |
| 3 | Kiáº¿n trÃºc há»‡ thá»‘ng |
| 4 | MÃ´ hÃ¬nh PhoBERT |
| 5 | Self-Attention |
| 6 | Luá»“ng xá»­ lÃ½ & Softmax |
| 7 | Fine-tuning |
| 8 | Káº¿t quáº£ thá»±c nghiá»‡m |
| 9 | Demo |
| 10 | Káº¿t luáº­n |

---

# SLIDE 1: TRANG BÃŒA

```
Há»† THá»NG PHÃ‚N TÃCH Cáº¢M XÃšC TRÃŠN Máº NG XÃƒ Há»˜I
Sá»¬ Dá»¤NG MÃ” HÃŒNH PHOBERT

Sinh viÃªn: [Há» tÃªn]
MSSV: [MÃ£ sá»‘]
GVHD: [TÃªn GVHD]

[TÃªn trÆ°á»ng] - 2025
```

---

# SLIDE 2: Tá»”NG QUAN & Má»¤C TIÃŠU

**Script:**
> "Äá» tÃ i xÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n tÃ­ch cáº£m xÃºc tá»« bÃ¬nh luáº­n tiáº¿ng Viá»‡t, sá»­ dá»¥ng mÃ´ hÃ¬nh PhoBERT."

**Ná»™i dung:**

```
Váº¤N Äá»€:
â€¢ 70+ triá»‡u ngÆ°á»i dÃ¹ng MXH táº¡i Viá»‡t Nam
â€¢ HÃ ng triá»‡u bÃ¬nh luáº­n má»—i ngÃ y
â€¢ KhÃ´ng thá»ƒ Ä‘á»c thá»§ cÃ´ng

GIáº¢I PHÃP:
â€¢ PhÃ¢n tÃ­ch cáº£m xÃºc tá»± Ä‘á»™ng báº±ng AI
â€¢ Sá»­ dá»¥ng PhoBERT - mÃ´ hÃ¬nh NLP tiáº¿ng Viá»‡t

Má»¤C TIÃŠU:
âœ… PhÃ¢n loáº¡i: TÃ­ch cá»±c ğŸ˜Š | Trung tÃ­nh ğŸ˜ | TiÃªu cá»±c ğŸ˜Ÿ
âœ… Há»‡ thá»‘ng web hoÃ n chá»‰nh
âœ… Äá»™ chÃ­nh xÃ¡c cao
```

---

# SLIDE 3: KIáº¾N TRÃšC Há»† THá»NG

**Script:**
> "Há»‡ thá»‘ng gá»“m 3 táº§ng: Frontend React vá»›i TailwindCSS, Backend FastAPI tÃ­ch há»£p PhoBERT, vÃ  Database MongoDB."

**Ná»™i dung:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          FRONTEND (React.js)            â”‚
â”‚     Giao diá»‡n + Biá»ƒu Ä‘á»“ trá»±c quan       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ REST API
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          BACKEND (FastAPI)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         PHOBERT MODEL             â”‚  â”‚
â”‚  â”‚    (12 layers, 768 hidden)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DATABASE (MongoDB)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


CÃ”NG NGHá»† Sá»¬ Dá»¤NG:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ React 18.2.0          - UI Framework                  â”‚
â”‚ â€¢ TailwindCSS 3.4.0     - CSS Framework                 â”‚
â”‚ â€¢ Chart.js 4.4.1        - Biá»ƒu Ä‘á»“ trá»±c quan             â”‚
â”‚ â€¢ Axios 1.6.2           - HTTP Client                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ FastAPI 0.104.1       - Web Framework (Python)        â”‚
â”‚ â€¢ Uvicorn 0.24.0        - ASGI Server                   â”‚
â”‚ â€¢ PyTorch 2.0+          - Deep Learning Framework       â”‚
â”‚ â€¢ Transformers 4.30+    - Hugging Face NLP              â”‚
â”‚ â€¢ Pydantic 2.5.0        - Data Validation               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI MODEL                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ PhoBERT-base          - Pre-trained Vietnamese LLM    â”‚
â”‚ â€¢ vinai/phobert-base    - Hugging Face Model            â”‚
â”‚ â€¢ Fine-tuned on UIT-ViHSD dataset                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATABASE & TOOLS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ MongoDB               - NoSQL Database                â”‚
â”‚ â€¢ Motor 3.3.2           - Async MongoDB Driver          â”‚
â”‚ â€¢ YouTube Comment API   - Láº¥y bÃ¬nh luáº­n tá»« YouTube      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# SLIDE 4: MÃ” HÃŒNH PHOBERT

**Script:**
> "PhoBERT lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ tiá»n huáº¥n luyá»‡n Ä‘áº§u tiÃªn dÃ nh riÃªng cho tiáº¿ng Viá»‡t, Ä‘Æ°á»£c VinAI Research phÃ¡t triá»ƒn nÄƒm 2020. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 20GB dá»¯ liá»‡u tiáº¿ng Viá»‡t tá»« bÃ¡o Ä‘iá»‡n tá»­ vÃ  Wikipedia, Ä‘áº¡t káº¿t quáº£ state-of-the-art trÃªn nhiá»u tÃ¡c vá»¥ NLP tiáº¿ng Viá»‡t."

**Ná»™i dung:**

```
PHOBERT (Pre-trained language model for Vietnamese)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ PhÃ¡t triá»ƒn: VinAI Research (2020)
â€¢ Nguá»“n: github.com/VinAIResearch/PhoBERT
â€¢ Paper: "PhoBERT: Pre-trained language models for Vietnamese"
â€¢ Kiáº¿n trÃºc: RoBERTa (Robustly Optimized BERT)

Dá»® LIá»†U HUáº¤N LUYá»†N:
â€¢ 20GB vÄƒn báº£n tiáº¿ng Viá»‡t
â€¢ Nguá»“n: BÃ¡o Ä‘iá»‡n tá»­ + Wikipedia tiáº¿ng Viá»‡t
â€¢ Tokenizer: BPE (Byte-Pair Encoding) vá»›i RDRSegmenter

THÃ”NG Sá» MÃ” HÃŒNH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ThÃ´ng sá»‘             â”‚ PhoBERT-baseâ”‚PhoBERT-largeâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sá»‘ layers            â”‚ 12         â”‚ 24         â”‚
â”‚ Hidden size          â”‚ 768        â”‚ 1024       â”‚
â”‚ Attention heads      â”‚ 12         â”‚ 16         â”‚
â”‚ FFN size             â”‚ 3072       â”‚ 4096       â”‚
â”‚ Tá»•ng tham sá»‘         â”‚ 135M       â”‚ 370M       â”‚
â”‚ Max sequence length  â”‚ 256        â”‚ 256        â”‚
â”‚ Vocab size           â”‚ 64,000     â”‚ 64,000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Táº I SAO PHOBERT CHO TIáº¾NG VIá»†T?
â€¢ BERT gá»‘c (Google): Huáº¥n luyá»‡n trÃªn tiáº¿ng Anh
  â†’ KhÃ´ng hiá»ƒu Ä‘áº·c trÆ°ng tiáº¿ng Viá»‡t
  
â€¢ Tiáº¿ng Viá»‡t cÃ³ Ä‘áº·c thÃ¹:
  - Thanh Ä‘iá»‡u (6 thanh: sáº¯c, huyá»n, há»i, ngÃ£, náº·ng, ngang)
  - Tá»« ghÃ©p (dáº¥u cÃ¡ch giá»¯a cÃ¡c Ã¢m tiáº¿t)
  - VÃ­ dá»¥: "khÃ´ng" + "tá»‘t" â‰  "khÃ´ng tá»‘t"
  
â€¢ PhoBERT: Hiá»ƒu ngá»¯ cáº£nh tiáº¿ng Viá»‡t tá»‘t hÆ¡n
```

---

# SLIDE 4.1: KIáº¾N TRÃšC CHI TIáº¾T PHOBERT

**Script:**
> "Kiáº¿n trÃºc PhoBERT gá»“m 3 pháº§n chÃ­nh: Embedding Layer, 12 Transformer Encoder Layers, vÃ  Classification Head."

**Ná»™i dung:**

```
KIáº¾N TRÃšC PHOBERT:

INPUT: "Ná»™i dung nÃ y ráº¥t hay"
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EMBEDDING LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Token Embedding (64,000 Ã— 768)                 â”‚ â”‚
â”‚  â”‚ + Position Embedding (256 Ã— 768)               â”‚ â”‚
â”‚  â”‚ = Input Embedding (seq_len Ã— 768)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           12 Ã— TRANSFORMER ENCODER LAYER            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Multi-Head Self-Attention (12 heads)           â”‚ â”‚
â”‚  â”‚     â†“                                          â”‚ â”‚
â”‚  â”‚ Add & Layer Normalization                      â”‚ â”‚
â”‚  â”‚     â†“                                          â”‚ â”‚
â”‚  â”‚ Feed-Forward Network (768 â†’ 3072 â†’ 768)        â”‚ â”‚
â”‚  â”‚     â†“                                          â”‚ â”‚
â”‚  â”‚ Add & Layer Normalization                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                    Ã— 12 láº§n                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CLASSIFICATION HEAD                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ [CLS] token â†’ Pooler (768 â†’ 768)               â”‚ â”‚
â”‚  â”‚              â†“                                 â”‚ â”‚
â”‚  â”‚         Dropout (0.1)                          â”‚ â”‚
â”‚  â”‚              â†“                                 â”‚ â”‚
â”‚  â”‚     Linear Layer (768 â†’ 3)                     â”‚ â”‚
â”‚  â”‚              â†“                                 â”‚ â”‚
â”‚  â”‚         Softmax                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
OUTPUT: [P(clean), P(offensive), P(hate)]
           0.92      0.05         0.03
          â†’ Prediction: CLEAN (92%)
```

---

# SLIDE 5: CÆ  CHáº¾ SELF-ATTENTION

**Script:**
> "Äiá»ƒm máº¡nh cá»§a PhoBERT lÃ  cÆ¡ cháº¿ Self-Attention, cho phÃ©p má»—i tá»« 'nhÃ¬n' vÃ o táº¥t cáº£ cÃ¡c tá»« khÃ¡c Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh."

**Ná»™i dung:**

```
SELF-ATTENTION = Má»—i tá»« chÃº Ã½ Ä‘áº¿n táº¥t cáº£ tá»« khÃ¡c

VÃ Dá»¤: "Sáº£n pháº©m khÃ´ng tá»‘t"

Ma tráº­n Attention:
              Sáº£n   pháº©m  khÃ´ng  tá»‘t
Sáº£n         [0.4   0.3   0.2   0.1]
pháº©m        [0.3   0.4   0.2   0.1]
khÃ´ng       [0.1   0.2   0.3   0.4] â† chÃº Ã½ "tá»‘t"
tá»‘t         [0.1   0.2   0.5   0.2] â† chÃº Ã½ "khÃ´ng"

â†’ MÃ´ hÃ¬nh hiá»ƒu: "khÃ´ng" + "tá»‘t" = TIÃŠU Cá»°C


CÃ”NG THá»¨C:
Attention(Q, K, V) = softmax(QK^T / âˆšd) Ã— V

â€¢ Q (Query): Tá»« hiá»‡n táº¡i Ä‘ang tÃ¬m gÃ¬?
â€¢ K (Key): CÃ¡c tá»« khÃ¡c cÃ³ thÃ´ng tin gÃ¬?
â€¢ V (Value): ThÃ´ng tin thá»±c sá»± cá»§a tá»«
```

---

# SLIDE 6: QUY TRÃŒNH PHÃ‚N TÃCH COMMENT

**Script:**
> "Khi ngÆ°á»i dÃ¹ng nháº­p má»™t bÃ¬nh luáº­n, há»‡ thá»‘ng sáº½ xá»­ lÃ½ qua 5 bÆ°á»›c chÃ­nh: tiá»n xá»­ lÃ½ vÄƒn báº£n, tokenization, embedding, xá»­ lÃ½ qua 12 transformer layers, vÃ  cuá»‘i cÃ¹ng lÃ  softmax Ä‘á»ƒ ra xÃ¡c suáº¥t phÃ¢n loáº¡i."

**Ná»™i dung:**

```
VÃ Dá»¤ THá»°C Táº¾:
INPUT: "Tháº±ng nÃ y ngu quÃ¡, Ä‘á»“ Ã³c chÃ³!"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÆ¯á»šC 1: TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N (Preprocessing)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
HÃ m: preprocess_text() trong sentiment_analyzer.py

â€¢ Chuyá»ƒn chá»¯ thÆ°á»ng: "tháº±ng nÃ y ngu quÃ¡ Ä‘á»“ Ã³c chÃ³"
â€¢ XÃ³a URL/link: (khÃ´ng cÃ³)
â€¢ XÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t: "tháº±ng nÃ y ngu quÃ¡ Ä‘á»“ Ã³c chÃ³"
â€¢ Chuáº©n hÃ³a khoáº£ng tráº¯ng: "tháº±ng nÃ y ngu quÃ¡ Ä‘á»“ Ã³c chÃ³"

Code thá»±c táº¿:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ text = text.lower()                                â”‚
â”‚ text = re.sub(r'http\S+|www.\S+', '', text)        â”‚
â”‚ text = re.sub(r'[^\w\sÃ Ã¡áº¡áº£Ã£...]', '', text)        â”‚
â”‚ text = ' '.join(text.split())                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÆ¯á»šC 2: TOKENIZATION (PhÃ¢n tÃ¡ch tá»«)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Tokenizer: PhoBERT BPE Tokenizer

Input:  "tháº±ng nÃ y ngu quÃ¡ Ä‘á»“ Ã³c chÃ³"
         â†“
Tokens: ["<s>", "tháº±ng", "nÃ y", "ngu", "quÃ¡", "Ä‘á»“", "Ã³c", "chÃ³", "</s>"]
         â†“
IDs:    [0, 2156, 89, 4523, 178, 892, 1045, 3421, 2]

â€¢ <s>: Token báº¯t Ä‘áº§u (CLS)
â€¢ </s>: Token káº¿t thÃºc (SEP)
â€¢ Má»—i tá»« â†’ 1 ID sá»‘ duy nháº¥t trong vocab (64,000 tá»«)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÆ¯á»šC 3: EMBEDDING (Chuyá»ƒn thÃ nh vector)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Token IDs â†’ Embedding Vectors (768 chiá»u)

Token "ngu" (ID: 4523):
â†’ Vector: [0.23, -0.15, 0.87, 0.02, ..., 0.45]  (768 sá»‘ thá»±c)

Tá»•ng embedding:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding     (768 chiá»u)         â”‚
â”‚ + Position Embedding (768 chiá»u)        â”‚
â”‚ = Final Embedding    (768 chiá»u)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÆ¯á»šC 4: 12 TRANSFORMER ENCODER LAYERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Má»—i layer thá»±c hiá»‡n:

[Multi-Head Self-Attention]
    Tá»« "ngu" chÃº Ã½ Ä‘áº¿n: "tháº±ng" (0.3), "Ã³c chÃ³" (0.4)
    â†’ Hiá»ƒu ngá»¯ cáº£nh: Ä‘Ã¢y lÃ  lá»i chá»­i/xÃºc pháº¡m
         â†“
[Add & Layer Normalization]
         â†“
[Feed-Forward Network]
    768 â†’ 3072 â†’ 768 (phi tuyáº¿n ReLU)
         â†“
[Add & Layer Normalization]

Láº·p láº¡i 12 láº§n â†’ Output: [CLS] token chá»©a thÃ´ng tin toÃ n cÃ¢u

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BÆ¯á»šC 5: CLASSIFICATION HEAD + SOFTMAX
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[CLS] vector (768 chiá»u)
         â†“
    [Dropout 0.1]
         â†“
    [Linear: 768 â†’ 3]
         â†“
    logits = [-0.5, 0.8, 2.3]
              clean  offensive  hate
         â†“
    [Softmax]
         â†“
    P(clean)     = exp(-0.5) / Î£ = 0.0605 / 0.1217 = 5.0%
    P(offensive) = exp(0.8)  / Î£ = 0.2231 / 0.1217 = 18.3%
    P(hate)      = exp(2.3)  / Î£ = 0.9974 / 0.1217 = 76.7%
         â†“
    Prediction = argmax â†’ HATE (76.7%)
```

---

# SLIDE 7: DATASET & FINE-TUNING

**Script:**
> "Em Ä‘Ã£ sá»­ dá»¥ng táº­p dá»¯ liá»‡u UIT-ViHSD - má»™t bá»™ dá»¯ liá»‡u phÃ¢n loáº¡i cáº£m xÃºc tiáº¿ng Viá»‡t tá»« cÃ¡c bÃ¬nh luáº­n cá»§a sinh viÃªn vá» pháº£n há»“i mÃ´n há»c. Táº­p dá»¯ liá»‡u Ä‘Æ°á»£c chia thÃ nh 3 pháº§n Ä‘á»ƒ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh."

**Ná»™i dung:**

```
DATASET: UIT-ViHSD (Vietnamese Hate Speech Detection)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Nguá»“n: BÃ¬nh luáº­n tá»« máº¡ng xÃ£ há»™i tiáº¿ng Viá»‡t
â€¢ NgÃ´n ngá»¯: Tiáº¿ng Viá»‡t
â€¢ NhÃ£n: 3 lá»›p (0 - Clean, 1 - Offensive, 2 - Hate)

PHÃ‚N CHIA Dá»® LIá»†U:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Táº­p dá»¯ liá»‡u  â”‚ Sá»‘ máº«u   â”‚ Tá»· lá»‡   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Train        â”‚ 24,775   â”‚ 72.5%   â”‚
â”‚ Validation   â”‚ 2,679    â”‚ 7.8%    â”‚
â”‚ Test         â”‚ 6,691    â”‚ 19.7%   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tá»”NG         â”‚ 34,145   â”‚ 100%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHÃ‚N Bá» NHÃƒN (Train set):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NhÃ£n         â”‚ Sá»‘ máº«u   â”‚ Tá»· lá»‡   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0 - Clean    â”‚ 19,886   â”‚ 82.7%   â”‚
â”‚ 1 - Offensiveâ”‚ 1,606    â”‚ 6.7%    â”‚
â”‚ 2 - Hate     â”‚ 2,556    â”‚ 10.6%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# SLIDE 7.1: QUÃ TRÃŒNH FINE-TUNING

**Script:**
> "Em Ä‘Ã£ fine-tune PhoBERT trÃªn táº­p dá»¯ liá»‡u nÃ y vá»›i cÃ¡c hyperparameters sau."

**Ná»™i dung:**

```
FINE-TUNING = Tinh chá»‰nh model cho bÃ i toÃ¡n cá»¥ thá»ƒ

PRE-TRAINED PHOBERT (VinAI)
    â€¢ ÄÃ£ hiá»ƒu ngÃ´n ngá»¯ tiáº¿ng Viá»‡t
    â€¢ ChÆ°a biáº¿t phÃ¢n loáº¡i cáº£m xÃºc
         â”‚
         â–¼ + Táº­p dá»¯ liá»‡u cÃ³ nhÃ£n (UIT-ViSHD)
         â”‚
FINE-TUNED MODEL
    â€¢ Biáº¿t phÃ¢n loáº¡i: Clean/Offensive/Hate


HYPERPARAMETERS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Epochs                  â”‚ 10        â”‚
â”‚ Batch size              â”‚ 16        â”‚
â”‚ Learning rate           â”‚ 2e-5      â”‚
â”‚ Max length              â”‚ 256       â”‚
â”‚ Warmup steps            â”‚ 500       â”‚
â”‚ Optimizer               â”‚ AdamW     â”‚
â”‚ Weight decay            â”‚ 0.01      â”‚
â”‚ FP16 (Mixed Precision)  â”‚ True      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# SLIDE 8: Káº¾T QUáº¢ THá»°C NGHIá»†M

**Script:**
> "Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c trÃªn táº­p test cho tháº¥y mÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c 86.74% vÃ  F1-score 86.25%."

**Ná»™i dung:**

```
Káº¾T QUáº¢ TRÃŠN Táº¬P TEST (sau 10 epochs):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Accuracy       â”‚ 86.74%    â”‚
â”‚ Precision      â”‚ 86.02%    â”‚
â”‚ Recall         â”‚ 86.74%    â”‚
â”‚ F1-Score       â”‚ 86.25%    â”‚
â”‚ Loss           â”‚ 0.7396    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ÄÃNH GIÃ:
âœ… Accuracy > 85% â†’ Káº¿t quáº£ tá»‘t
âœ… F1-Score cÃ¢n báº±ng giá»¯a Precision vÃ  Recall
âœ… Model khÃ´ng bá»‹ overfitting


THÃ”NG TIN HUáº¤N LUYá»†N:
â€¢ Epochs: 10
â€¢ Tá»‘c Ä‘á»™: ~295 samples/giÃ¢y
â€¢ Thá»i gian Ä‘Ã¡nh giÃ¡: ~22.6 giÃ¢y
```

---

# SLIDE 9: DEMO Há»† THá»NG

**Script:**
> "Em xin demo há»‡ thá»‘ng. NgÆ°á»i dÃ¹ng nháº­p bÃ¬nh luáº­n, há»‡ thá»‘ng tráº£ vá» nhÃ£n cáº£m xÃºc vÃ  Ä‘á»™ tin cáº­y."

**Ná»™i dung:**

```
DEMO TRá»°C TIáº¾P

1. Nháº­p bÃ¬nh luáº­n:
   "Sáº£n pháº©m nÃ y quÃ¡ tuyá»‡t vá»i, ráº¥t Ä‘Ã¡ng mua!"

2. Káº¿t quáº£:
   âœ… Cáº£m xÃºc: POSITIVE
   âœ… Äá»™ tin cáº­y: 95%
   âœ… Biá»ƒu Ä‘á»“ thá»‘ng kÃª

3. TÃ­nh nÄƒng:
   â€¢ PhÃ¢n tÃ­ch nhiá»u bÃ¬nh luáº­n cÃ¹ng lÃºc
   â€¢ PhÃ¢n tÃ­ch tá»« URL YouTube
   â€¢ PhÃ¡t hiá»‡n ná»™i dung nháº¡y cáº£m
   â€¢ Äá» xuáº¥t cho ngÆ°á»i dÃ¹ng
```

*[Demo trá»±c tiáº¿p]*

---

# SLIDE 10: Káº¾T LUáº¬N

**Script:**
> "Äá» tÃ i Ä‘Ã£ hoÃ n thÃ nh cÃ¡c má»¥c tiÃªu Ä‘á» ra. Cáº£m Æ¡n Há»™i Ä‘á»“ng Ä‘Ã£ láº¯ng nghe."

**Ná»™i dung:**

```
Káº¾T QUáº¢ Äáº T ÄÆ¯á»¢C:
âœ… Fine-tune PhoBERT cho phÃ¢n tÃ­ch cáº£m xÃºc tiáº¿ng Viá»‡t
âœ… Há»‡ thá»‘ng web hoÃ n chá»‰nh (React + FastAPI + MongoDB)
âœ… Äá»™ chÃ­nh xÃ¡c: [XX]%

Háº N CHáº¾:
âš ï¸ ChÆ°a há»— trá»£ Facebook, TikTok
âš ï¸ KhÃ³ xá»­ lÃ½ tiáº¿ng lÃ³ng, viáº¿t táº¯t

HÆ¯á»šNG PHÃT TRIá»‚N:
ğŸš€ TÃ­ch há»£p thÃªm nhiá»u ná»n táº£ng
ğŸš€ PhÃ¢n tÃ­ch real-time
ğŸš€ Mobile app

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cáº¢M Æ N QUÃ THáº¦Y CÃ” ÄÃƒ Láº®NG NGHE!

Q&A
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

# PHá»¤ Lá»¤C: CÃ‚U Há»I & TRáº¢ Lá»œI

**Q1: Táº¡i sao chá»n PhoBERT?**
> PhoBERT Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 20GB tiáº¿ng Viá»‡t, hiá»ƒu Ä‘áº·c trÆ°ng ngÃ´n ngá»¯ Viá»‡t hÆ¡n BERT tiáº¿ng Anh.

**Q2: Táº¡i sao 12 layers?**
> 12 layers cÃ¢n báº±ng giá»¯a accuracy vÃ  tá»‘c Ä‘á»™. BERT-large 24 layers chá»‰ tá»‘t hÆ¡n 1-2% nhÆ°ng cháº­m gáº¥p Ä‘Ã´i.

**Q3: Self-Attention hoáº¡t Ä‘á»™ng tháº¿ nÃ o?**
> Má»—i tá»« tÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng vá»›i táº¥t cáº£ tá»« khÃ¡c, tá»« nÃ o Ä‘iá»ƒm cao thÃ¬ "chÃº Ã½" nhiá»u hÆ¡n.

**Q4: Softmax lÃ  gÃ¬?**
> Softmax chuyá»ƒn Ä‘iá»ƒm thÃ´ thÃ nh xÃ¡c suáº¥t (tá»•ng = 100%), láº¥y xÃ¡c suáº¥t cao nháº¥t lÃ m káº¿t quáº£.

---

*ChÃºc báº¡n báº£o vá»‡ thÃ nh cÃ´ng! ğŸ“*
