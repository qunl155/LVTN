# ğŸ“š LÃ THUYáº¾T Äáº¦Y Äá»¦: MÃ” HÃŒNH PHOBERT PHÃ‚N TÃCH Cáº¢M XÃšC

---

# PHáº¦N A: Ná»€N Táº¢NG LÃ THUYáº¾T

---

## 1. Xá»¬ LÃ NGÃ”N NGá»® Tá»° NHIÃŠN (NLP)

### 1.1 NLP lÃ  gÃ¬?

**NLP (Natural Language Processing)** = Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn

```
Äá»‹nh nghÄ©a:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NLP lÃ  nhÃ¡nh cá»§a TrÃ­ tuá»‡ nhÃ¢n táº¡o (AI) giÃºp mÃ¡y tÃ­nh
hiá»ƒu, phÃ¢n tÃ­ch vÃ  táº¡o ra ngÃ´n ngá»¯ cá»§a con ngÆ°á»i.
```

### 1.2 CÃ¡c bÃ i toÃ¡n NLP phá»• biáº¿n

| BÃ i toÃ¡n | MÃ´ táº£ | VÃ­ dá»¥ |
|:---------|:------|:------|
| PhÃ¢n loáº¡i vÄƒn báº£n | GÃ¡n nhÃ£n cho vÄƒn báº£n | Spam/KhÃ´ng spam |
| PhÃ¢n tÃ­ch cáº£m xÃºc | XÃ¡c Ä‘á»‹nh cáº£m xÃºc | TÃ­ch cá»±c/TiÃªu cá»±c |
| Nháº­n dáº¡ng thá»±c thá»ƒ | TÃ¬m tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm | "HÃ  Ná»™i" â†’ Äá»‹a Ä‘iá»ƒm |
| Dá»‹ch mÃ¡y | Dá»‹ch ngÃ´n ngá»¯ | Viá»‡t â†’ Anh |
| Há»i Ä‘Ã¡p | Tráº£ lá»i cÃ¢u há»i | Chatbot |
| TÃ³m táº¯t vÄƒn báº£n | RÃºt gá»n ná»™i dung | TÃ³m táº¯t bÃ¡o |

### 1.3 ThÃ¡ch thá»©c cá»§a NLP

```
1. TÃNH ÄA NGHÄ¨A
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ "Bank" = NgÃ¢n hÃ ng hay Bá» sÃ´ng?
   â€¢ "ÄÃ¡nh" = ÄÃ¡nh Ä‘Ã n hay ÄÃ¡nh nhau?

2. NGá»® Cáº¢NH
   â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ "KhÃ´ng tá»‡" = Tá»‘t (phá»§ Ä‘á»‹nh cá»§a phá»§ Ä‘á»‹nh)
   â€¢ "Tá»‘t tháº­t Ä‘áº¥y" = CÃ³ thá»ƒ lÃ  má»‰a mai

3. TIáº¾NG VIá»†T Äáº¶C BIá»†T
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ KhÃ´ng cÃ³ dáº¥u cÃ¡ch giá»¯a cÃ¡c tá»« ghÃ©p: "há»c sinh"
   â€¢ Thanh Ä‘iá»‡u thay Ä‘á»•i nghÄ©a: "ma", "mÃ¡", "mÃ ", "máº£", "mÃ£", "máº¡"
   â€¢ Tá»« Ä‘á»“ng Ã¢m nhiá»u: "sao" (sao chÃ©p, ngÃ´i sao, táº¡i sao)
```

---

## 2. DEEP LEARNING (Há»ŒC SÃ‚U)

### 2.1 Neural Network (Máº¡ng nÆ¡-ron)

```
NEURON NHÃ‚N Táº O
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MÃ´ phá»ng cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a táº¿ bÃ o tháº§n kinh trong nÃ£o

        x1 â”€â”€â”¬â”€â”€(w1)â”€â”€â”
              â”‚        â”‚
        x2 â”€â”€â”¼â”€â”€(w2)â”€â”€â”¼â”€â”€â†’ [Î£] â†’ [f] â†’ output
              â”‚        â”‚
        x3 â”€â”€â”´â”€â”€(w3)â”€â”€â”˜

Trong Ä‘Ã³:
â€¢ x1, x2, x3 = CÃ¡c Ä‘áº§u vÃ o (inputs)
â€¢ w1, w2, w3 = Trá»ng sá»‘ (weights) - má»©c Ä‘á»™ quan trá»ng
â€¢ Î£ = Tá»•ng cÃ³ trá»ng sá»‘ = x1Ã—w1 + x2Ã—w2 + x3Ã—w3 + bias
â€¢ f = HÃ m kÃ­ch hoáº¡t (activation function)
â€¢ output = Káº¿t quáº£ Ä‘áº§u ra
```

### 2.2 CÃ´ng thá»©c Neuron

```
CÃ”NG THá»¨C CÆ  Báº¢N:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
output = f(w1Ã—x1 + w2Ã—x2 + ... + wnÃ—xn + b)
       = f(Î£ wixi + b)

Trong Ä‘Ã³:
â€¢ wi = Trá»ng sá»‘ thá»© i (weight)
â€¢ xi = Äáº§u vÃ o thá»© i (input)
â€¢ b  = Äá»™ lá»‡ch (bias)
â€¢ f  = HÃ m kÃ­ch hoáº¡t
```

### 2.3 CÃ¡c hÃ m kÃ­ch hoáº¡t phá»• biáº¿n

```
1. SIGMOID
   â”€â”€â”€â”€â”€â”€â”€â”€
   f(x) = 1 / (1 + e^(-x))
   
   â€¢ Output: 0 Ä‘áº¿n 1
   â€¢ DÃ¹ng cho: XÃ¡c suáº¥t, phÃ¢n loáº¡i nhá»‹ phÃ¢n
   
   Äá»“ thá»‹:
   1 â”¤        â•­â”€â”€â”€â”€
     â”‚      â•±
 0.5â”¤â”€â”€â”€â”€â•±
     â”‚  â•±
   0 â”¤â•±
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


2. RELU (Rectified Linear Unit)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   f(x) = max(0, x)
   
   â€¢ Output: 0 hoáº·c x (náº¿u x > 0)
   â€¢ DÃ¹ng cho: Hidden layers
   
   Äá»“ thá»‹:
     â”‚    â•±
     â”‚  â•±
     â”‚â•±
   â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚


3. SOFTMAX
   â”€â”€â”€â”€â”€â”€â”€â”€
   f(xi) = e^(xi) / Î£e^(xj)
   
   â€¢ Output: XÃ¡c suáº¥t (tá»•ng = 1)
   â€¢ DÃ¹ng cho: PhÃ¢n loáº¡i nhiá»u lá»›p


4. GELU (Ä‘Æ°á»£c dÃ¹ng trong BERT)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   f(x) â‰ˆ 0.5Ã—xÃ—(1 + tanh(0.7978Ã—(x + 0.0356Ã—xÂ³)))
   
   â€¢ MÆ°á»£t hÆ¡n ReLU
   â€¢ Hiá»‡u quáº£ cho NLP
```

### 2.4 Deep Neural Network

```
Máº NG NHIá»€U Lá»šP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input     Hidden Layer 1    Hidden Layer 2    Output
Layer

 [x1]         [h1]              [h3]           [y1]
   â•²         â•±    â•²            â•±    â•²         â•±
    â•²       â•±      â•²          â•±      â•²       â•±
 [x2]â”€â”€â”€â”€â”€[h2]â”€â”€â”€â”€â”€â”€[h4]â”€â”€â”€â”€â”€[y2]
    â•±       â•²      â•±          â•²      â•±       â•²
   â•±         â•²    â•±            â•²    â•±         â•²
 [x3]         [h5]              [h5]           [y3]

â€¢ "Deep" = CÃ³ NHIá»€U hidden layers (cÃ ng nhiá»u = cÃ ng sÃ¢u)
â€¢ PhoBERT cÃ³ 12 layers â†’ "deep learning"
```

### 2.5 QuÃ¡ trÃ¬nh huáº¥n luyá»‡n

```
TRAINING LOOP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. FORWARD PASS (Truyá»n xuÃ´i)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Input â†’ Model â†’ Dá»± Ä‘oÃ¡n (Prediction)

2. TÃNH LOSS (Máº¥t mÃ¡t)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Loss = Äo sá»± khÃ¡c biá»‡t giá»¯a Dá»± Ä‘oÃ¡n vÃ  NhÃ£n thá»±c

3. BACKWARD PASS (Truyá»n ngÆ°á»£c - Backpropagation)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   TÃ­nh gradient (Ä‘áº¡o hÃ m) cá»§a Loss theo tá»«ng weight

4. Cáº¬P NHáº¬T WEIGHTS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   weight_má»›i = weight_cÅ© - learning_rate Ã— gradient

5. Láº¶P Láº I
   â”€â”€â”€â”€â”€â”€â”€â”€
   Láº·p láº¡i bÆ°á»›c 1-4 cho Ä‘áº¿n khi Loss Ä‘á»§ nhá»
```

---

## 3. WORD EMBEDDINGS (NHÃšNG Tá»ª)

### 3.1 Táº¡i sao cáº§n Embedding?

```
Váº¤N Äá»€: MÃ¡y tÃ­nh chá»‰ hiá»ƒu sá»‘, khÃ´ng hiá»ƒu chá»¯

CÃCH CÅ¨: One-Hot Encoding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tá»« Ä‘iá»ƒn: ["mÃ¨o", "chÃ³", "cÃ¡"]

"mÃ¨o" = [1, 0, 0]
"chÃ³" = [0, 1, 0]
"cÃ¡"  = [0, 0, 1]

NhÆ°á»£c Ä‘iá»ƒm:
â€¢ Vector quÃ¡ dÃ i (tá»« Ä‘iá»ƒn 50,000 tá»« = vector 50,000 chiá»u)
â€¢ KhÃ´ng thá»ƒ hiá»‡n quan há»‡ ngá»¯ nghÄ©a
â€¢ "mÃ¨o" vÃ  "chÃ³" cÃ³ khoáº£ng cÃ¡ch = "mÃ¨o" vÃ  "cÃ¡"


CÃCH Má»šI: Word Embedding
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"mÃ¨o" = [0.2, 0.5, -0.1, 0.8, ...]  (300 chiá»u)
"chÃ³" = [0.3, 0.4, -0.2, 0.7, ...]  (300 chiá»u)
"cÃ¡"  = [-0.5, 0.1, 0.9, -0.3, ...] (300 chiá»u)

Æ¯u Ä‘iá»ƒm:
â€¢ Vector ngáº¯n hÆ¡n (100-768 chiá»u)
â€¢ Thá»ƒ hiá»‡n ngá»¯ nghÄ©a
â€¢ "mÃ¨o" vÃ  "chÃ³" gáº§n nhau (Ä‘á»u lÃ  thÃº cÆ°ng)
```

### 3.2 TÃ­nh cháº¥t cá»§a Word Embedding

```
PHÃ‰P TOÃN Vá»šI Tá»ª
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

vector("vua") - vector("Ä‘Ã n Ã´ng") + vector("phá»¥ ná»¯") â‰ˆ vector("hoÃ ng háº­u")

Giáº£i thÃ­ch:
â€¢ "vua" = "Ä‘Ã n Ã´ng" + "quyá»n lá»±c hoÃ ng gia"
â€¢ Bá» "Ä‘Ã n Ã´ng", thÃªm "phá»¥ ná»¯" â†’ "hoÃ ng háº­u"


CÃC Tá»ª TÆ¯Æ NG Tá»° Náº°M Gáº¦N NHAU
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

     "tá»‘t" â—
      "hay" â—     â† CÃ¡c tá»« tÃ­ch cá»±c
    "tuyá»‡t" â—
                      
                      
                      "xáº¥u" â—
                    "tá»‡" â—     â† CÃ¡c tá»« tiÃªu cá»±c
                  "kÃ©m" â—
```

### 3.3 CÃ¡c phÆ°Æ¡ng phÃ¡p Embedding

| PhÆ°Æ¡ng phÃ¡p | NÄƒm | Äáº·c Ä‘iá»ƒm |
|:------------|:---:|:---------|
| Word2Vec | 2013 | Vector tÄ©nh cho má»—i tá»« |
| GloVe | 2014 | Dá»±a trÃªn ma tráº­n Ä‘á»“ng xuáº¥t hiá»‡n |
| FastText | 2016 | Xá»­ lÃ½ Ä‘Æ°á»£c tá»« chÆ°a gáº·p |
| ELMo | 2018 | Vector Ä‘á»™ng theo ngá»¯ cáº£nh |
| **BERT/PhoBERT** | 2018-2020 | Vector Ä‘á»™ng, hai chiá»u |

---

## 4. ATTENTION MECHANISM (CÆ  CHáº¾ CHÃš Ã)

### 4.1 Ã tÆ°á»Ÿng

```
Váº¤N Äá»€ CÅ¨
â”€â”€â”€â”€â”€â”€â”€â”€â”€
Khi dá»‹ch cÃ¢u dÃ i, model cÅ© pháº£i nhá»› toÃ n bá»™ cÃ¢u trong má»™t vector.
ThÃ´ng tin bá»‹ máº¥t khi cÃ¢u quÃ¡ dÃ i.

VÃ­ dá»¥ dá»‹ch: "Con mÃ¨o mÃ u Ä‘en Ä‘ang ngá»§ trÃªn gháº¿"

Model cÅ©:
[ToÃ n bá»™ cÃ¢u] â†’ [1 vector 256 chiá»u] â†’ [Dá»‹ch]
                     â†‘
              ThÃ´ng tin bá»‹ nÃ©n, máº¥t chi tiáº¿t


GIáº¢I PHÃP: ATTENTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Khi dá»‹ch tá»«ng tá»«, model "nhÃ¬n láº¡i" toÃ n bá»™ cÃ¢u gá»‘c
vÃ  CHÃš Ã Ä‘áº¿n cÃ¡c tá»« liÃªn quan nháº¥t.

Khi dá»‹ch "cat":
â€¢ ChÃº Ã½ nhiá»u Ä‘áº¿n "mÃ¨o" (0.8)
â€¢ ChÃº Ã½ Ã­t Ä‘áº¿n "Ä‘ang" (0.05)
â€¢ ChÃº Ã½ Ã­t Ä‘áº¿n "gháº¿" (0.1)
```

### 4.2 Self-Attention

```
SELF-ATTENTION = ChÃº Ã½ Ä‘áº¿n chÃ­nh mÃ¬nh
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CÃ¢u: "Con mÃ¨o khÃ´ng cáº¯n vÃ¬ nÃ³ hiá»n"

Khi xá»­ lÃ½ tá»« "nÃ³":
â€¢ Model cáº§n hiá»ƒu "nÃ³" Ã¡m chá»‰ "mÃ¨o"
â€¢ Self-attention giÃºp "nÃ³" chÃº Ã½ Ä‘áº¿n "mÃ¨o"

Trá»ng sá»‘ attention cá»§a "nÃ³":
â€¢ "Con" : 0.05
â€¢ "mÃ¨o" : 0.60  â† ChÃº Ã½ nhiá»u nháº¥t
â€¢ "khÃ´ng": 0.05
â€¢ "cáº¯n" : 0.10
â€¢ "vÃ¬"  : 0.05
â€¢ "nÃ³"  : 0.10
â€¢ "hiá»n": 0.05
```

### 4.3 CÃ´ng thá»©c Attention

```
BÆ¯á»šC 1: Táº¡o Query, Key, Value
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Query (Q) = "TÃ´i Ä‘ang tÃ¬m gÃ¬?"
â€¢ Key (K)   = "TÃ´i cÃ³ thÃ´ng tin gÃ¬?"
â€¢ Value (V) = "ThÃ´ng tin thá»±c sá»± cá»§a tÃ´i"

CÃ´ng thá»©c:
Q = X Ã— W_Q
K = X Ã— W_K
V = X Ã— W_V


BÆ¯á»šC 2: TÃ­nh Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Score = Q Ã— K^T

Ã nghÄ©a: Äiá»ƒm cao = Query vÃ  Key khá»›p nhau


BÆ¯á»šC 3: Chia tá»· lá»‡ (Scaling)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Score_scaled = Score Ã· cÄƒn(d_k)

Trong Ä‘Ã³ d_k = 64 (sá»‘ chiá»u cá»§a Key)


BÆ¯á»šC 4: Softmax
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weights = softmax(Score_scaled)

Chuyá»ƒn Ä‘iá»ƒm thÃ nh xÃ¡c suáº¥t (tá»•ng = 1)


BÆ¯á»šC 5: TÃ­nh Output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Output = Weights Ã— V

Láº¥y tá»•ng cÃ³ trá»ng sá»‘ cá»§a Values


CÃ”NG THá»¨C Tá»”NG Há»¢P:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Attention(Q, K, V) = softmax((Q Ã— K^T) Ã· cÄƒn(d_k)) Ã— V
```

---

## 5. TRANSFORMER ARCHITECTURE

### 5.1 Lá»‹ch sá»­

```
TRÆ¯á»šC TRANSFORMER (2017)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RNN (Recurrent Neural Network):
â€¢ Xá»­ lÃ½ tuáº§n tá»± tá»«ng tá»« má»™t
â€¢ Cháº­m, khÃ´ng thá»ƒ song song hÃ³a
â€¢ KhÃ³ nhá»› thÃ´ng tin xa

      x1 â†’ [RNN] â†’ h1
                    â†“
      x2 â†’ [RNN] â†’ h2
                    â†“
      x3 â†’ [RNN] â†’ h3  (thÃ´ng tin x1 cÃ³ thá»ƒ bá»‹ máº¥t)


SAU TRANSFORMER (2017 - nay)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Transformer:
â€¢ Xá»­ lÃ½ SONG SONG táº¥t cáº£ tá»« cÃ¹ng lÃºc
â€¢ Nhanh hÆ¡n RNN ráº¥t nhiá»u
â€¢ Self-attention giÃºp nhá»› má»i thá»©

      x1 â”€â”¬â”€â†’ [Attention] â”€â†’ h1
      x2 â”€â”¼â”€â†’ [Attention] â”€â†’ h2
      x3 â”€â”´â”€â†’ [Attention] â”€â†’ h3
              (xá»­ lÃ½ Ä‘á»“ng thá»i)
```

### 5.2 Kiáº¿n trÃºc Transformer

```
TRANSFORMER Gá»C (cho dá»‹ch mÃ¡y)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TRANSFORMER                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚    ENCODER     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚    DECODER     â”‚          â”‚
â”‚  â”‚                â”‚         â”‚                â”‚          â”‚
â”‚  â”‚ "I love cats"  â”‚         â”‚ "TÃ´i yÃªu mÃ¨o"  â”‚          â”‚
â”‚  â”‚                â”‚         â”‚                â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                          â”‚
â”‚  Encoder: Äá»c hiá»ƒu        Decoder: Táº¡o output           â”‚
â”‚           cÃ¢u gá»‘c                  tá»«ng tá»«              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


BERT/PhoBERT (chá»‰ dÃ¹ng Encoder)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BERT / PhoBERT                        â”‚
â”‚                                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚         â”‚       ENCODER          â”‚                       â”‚
â”‚         â”‚                        â”‚                       â”‚
â”‚         â”‚ "Sáº£n pháº©m ráº¥t tá»‘t"    â”‚                       â”‚
â”‚         â”‚         â†“              â”‚                       â”‚
â”‚         â”‚   [Hiá»ƒu ngá»¯ nghÄ©a]    â”‚                       â”‚
â”‚         â”‚         â†“              â”‚                       â”‚
â”‚         â”‚  Vector Ä‘áº¡i diá»‡n cÃ¢u   â”‚                       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                    â†“                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â”‚  CLASSIFICATION   â”‚                            â”‚
â”‚         â”‚  Positive/Negativeâ”‚                            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 Cáº¥u trÃºc má»™t lá»›p Transformer

```
                 Input
                   â”‚
                   â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Multi-Head      â”‚
         â”‚ Self-Attention  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–ºâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”  (Residual Connection)
         â”‚        â–¼        â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚ â”‚ Layer Norm  â”‚ â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚        â”‚        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Feed-Forward   â”‚
         â”‚     Network     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–ºâ”‚â—„â”€â”€â”€â”€â”€â”€â”€â”  (Residual Connection)
         â”‚        â–¼        â”‚
         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
         â”‚ â”‚ Layer Norm  â”‚ â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
         â”‚        â”‚        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
                Output
```

---

## 6. BERT (Bidirectional Encoder Representations from Transformers)

### 6.1 BERT lÃ  gÃ¬?

```
BERT = Bidirectional Encoder Representations from Transformers

Giáº£i thÃ­ch tá»«ng pháº§n:
â€¢ Bidirectional: Äá»c cáº£ hai chiá»u (trÃ¡iâ†’pháº£i VÃ€ pháº£iâ†’trÃ¡i)
â€¢ Encoder: Chá»‰ dÃ¹ng pháº§n Encoder cá»§a Transformer
â€¢ Representations: Táº¡o biá»ƒu diá»…n (vector) cho vÄƒn báº£n
â€¢ Transformers: Dá»±a trÃªn kiáº¿n trÃºc Transformer


SO SÃNH Vá»šI CÃC MODEL TRÆ¯á»šC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GPT (chá»‰ Ä‘á»c trÃ¡iâ†’pháº£i):
"TÃ´i yÃªu [MASK]" â†’ Model chá»‰ tháº¥y "TÃ´i yÃªu"

ELMo (káº¿t há»£p 2 chiá»u riÃªng biá»‡t):
TrÃ¡iâ†’pháº£i: "TÃ´i yÃªu" â†’ vector_1
Pháº£iâ†’trÃ¡i: "[MASK]" â†’ vector_2
Káº¿t há»£p: vector_1 + vector_2

BERT (Ä‘á»c cáº£ 2 chiá»u Äá»’NG THá»œI):
"TÃ´i yÃªu [MASK] cá»§a mÃ¬nh"
         â†‘
Model tháº¥y Cáº¢ "TÃ´i yÃªu" VÃ€ "cá»§a mÃ¬nh" â†’ hiá»ƒu ngá»¯ cáº£nh tá»‘t hÆ¡n
```

### 6.2 Pre-training Tasks (Nhiá»‡m vá»¥ huáº¥n luyá»‡n trÆ°á»›c)

```
BERT Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i 2 nhiá»‡m vá»¥:

1. MASKED LANGUAGE MODEL (MLM)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Che 15% cÃ¡c tá»« vÃ  yÃªu cáº§u model Ä‘oÃ¡n

   Input:  "TÃ´i [MASK] Äƒn phá»Ÿ ráº¥t ngon"
   Output: Model Ä‘oÃ¡n [MASK] = "thÃ­ch"

   Má»¥c Ä‘Ã­ch: Há»c ngá»¯ nghÄ©a cá»§a tá»« trong ngá»¯ cáº£nh


2. NEXT SENTENCE PREDICTION (NSP)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Cho 2 cÃ¢u, há»i cÃ¢u B cÃ³ theo sau cÃ¢u A khÃ´ng

   CÃ¢u A: "TÃ´i Ä‘Ã³i bá»¥ng"
   CÃ¢u B: "TÃ´i Ä‘i Äƒn cÆ¡m"
   â†’ ÄÃºng (IsNext)

   CÃ¢u A: "TÃ´i Ä‘Ã³i bá»¥ng"
   CÃ¢u B: "HÃ´m nay trá»i Ä‘áº¹p"
   â†’ Sai (NotNext)

   Má»¥c Ä‘Ã­ch: Há»c quan há»‡ giá»¯a cÃ¡c cÃ¢u
```

### 6.3 Fine-tuning (Tinh chá»‰nh)

```
PRE-TRAINING vs FINE-TUNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PRE-TRAINING (Huáº¥n luyá»‡n trÆ°á»›c):
â€¢ Dá»¯ liá»‡u: HÃ ng tá»· tá»« tá»« Wikipedia, sÃ¡ch, web
â€¢ Nhiá»‡m vá»¥: MLM + NSP
â€¢ Thá»i gian: Nhiá»u ngÃ y trÃªn nhiá»u GPU
â€¢ Ai lÃ m: Google, VinAI (cho PhoBERT)
â€¢ Káº¿t quáº£: Model hiá»ƒu ngÃ´n ngá»¯ chung

FINE-TUNING (Tinh chá»‰nh):
â€¢ Dá»¯ liá»‡u: Dataset cá»¥ thá»ƒ (vÃ­ dá»¥: 10,000 bÃ¬nh luáº­n cÃ³ nhÃ£n)
â€¢ Nhiá»‡m vá»¥: PhÃ¢n loáº¡i cáº£m xÃºc
â€¢ Thá»i gian: VÃ i giá» trÃªn 1 GPU
â€¢ Ai lÃ m: Báº¡n
â€¢ Káº¿t quáº£: Model chuyÃªn biá»‡t cho bÃ i toÃ¡n cá»§a báº¡n


QUÃ TRÃŒNH FINE-TUNING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Láº¥y PhoBERT pre-trained
   â†“
2. ThÃªm lá»›p Classification lÃªn trÃªn
   â†“
3. Huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u phÃ¢n loáº¡i cáº£m xÃºc
   â†“
4. Model há»c cÃ¡ch phÃ¢n loáº¡i cáº£m xÃºc

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        PHOBERT (Ä‘Ã£ pre-trained)     â”‚  â† Giá»¯ nguyÃªn hoáº·c fine-tune nháº¹
   â”‚    [Hiá»ƒu ngÃ´n ngá»¯ tiáº¿ng Viá»‡t]       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      CLASSIFICATION LAYER           â”‚  â† Huáº¥n luyá»‡n má»›i
   â”‚    [Positive/Neutral/Negative]      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. PHOBERT - BERT CHO TIáº¾NG VIá»†T

### 7.1 PhoBERT lÃ  gÃ¬?

```
PhoBERT = BERT Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº·c biá»‡t cho TIáº¾NG VIá»†T

PhÃ¡t triá»ƒn bá»Ÿi: VinAI Research (Viá»‡t Nam)
CÃ´ng bá»‘: NÄƒm 2020
Paper: "PhoBERT: Pre-trained language models for Vietnamese"
```

### 7.2 Táº¡i sao cáº§n PhoBERT?

```
Váº¤N Äá»€ Vá»šI BERT Gá»C (TIáº¾NG ANH)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. KhÃ´ng hiá»ƒu tiáº¿ng Viá»‡t
   â€¢ BERT: tá»« Ä‘iá»ƒn tiáº¿ng Anh
   â€¢ "xin chÃ o" â†’ tokens khÃ´ng cÃ³ nghÄ©a

2. Tokenization sai
   â€¢ BERT: "tuyá»‡t vá»i" â†’ ["tuy", "##á»‡t", "v", "##á»i"] (sai)
   â€¢ PhoBERT: "tuyá»‡t vá»i" â†’ ["tuyá»‡t", "vá»i"] (Ä‘Ãºng)

3. KhÃ´ng hiá»ƒu Ä‘áº·c trÆ°ng tiáº¿ng Viá»‡t
   â€¢ Thanh Ä‘iá»‡u
   â€¢ Tá»« ghÃ©p
   â€¢ CÃº phÃ¡p


PHOBERT GIáº¢I QUYáº¾T
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Huáº¥n luyá»‡n trÃªn 20GB dá»¯ liá»‡u tiáº¿ng Viá»‡t
   â€¢ BÃ¡o VnExpress, DÃ¢n TrÃ­, VNEconomy...
   â€¢ Wikipedia tiáº¿ng Viá»‡t
   
2. Sá»­ dá»¥ng pyvi vÃ  RDRSegmenter cho word segmentation
   â€¢ "há»c sinh" â†’ 1 token (khÃ´ng pháº£i 2)
   
3. Tá»« Ä‘iá»ƒn ~64,000 tokens tiáº¿ng Viá»‡t
```

### 7.3 Hai phiÃªn báº£n PhoBERT

| PhiÃªn báº£n | Layers | Hidden | Heads | Parameters |
|:----------|:------:|:------:|:-----:|:----------:|
| PhoBERT-base | 12 | 768 | 12 | 135M |
| PhoBERT-large | 24 | 1024 | 16 | 370M |

```
Project cá»§a báº¡n sá»­ dá»¥ng: PhoBERT-base (12 layers)
```

---

# PHáº¦N B: QUY TRÃŒNH Xá»¬ LÃ CHI TIáº¾T

---

## 8. TIá»€N Xá»¬ LÃ VÄ‚N Báº¢N

### 8.1 LÃ½ thuyáº¿t

```
Táº I SAO Cáº¦N TIá»€N Xá»¬ LÃ?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VÄƒn báº£n thá»±c táº¿ ráº¥t "báº©n":
â€¢ CHá»® HOA, chá»¯ thÆ°á»ng láº«n lá»™n
â€¢ URL, email, @mentions
â€¢ Emoji ğŸ‘ğŸ˜ŠğŸ”¥
â€¢ KÃ½ tá»± Ä‘áº·c biá»‡t !!!###
â€¢ Khoáº£ng tráº¯ng    thá»«a

Model há»c tá»‘t hÆ¡n vá»›i dá»¯ liá»‡u sáº¡ch vÃ  nháº¥t quÃ¡n.
```

### 8.2 CÃ¡c bÆ°á»›c xá»­ lÃ½

```
BÆ¯á»šC 1: LOWERCASE (Chá»¯ thÆ°á»ng)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  "Sáº¢N PHáº¨M Tá»T"
Output: "sáº£n pháº©m tá»‘t"

LÃ½ do: "Tá»T" vÃ  "tá»‘t" nÃªn Ä‘Æ°á»£c xá»­ lÃ½ nhÆ° nhau


BÆ¯á»šC 2: XÃ“A URL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  "Xem táº¡i https://example.com"
Output: "Xem táº¡i"

Pattern: http\S+ hoáº·c www.\S+
LÃ½ do: URL khÃ´ng mang ngá»¯ nghÄ©a cáº£m xÃºc


BÆ¯á»šC 3: XÃ“A KÃ Tá»° Äáº¶C BIá»†T
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  "Tá»‘t quÃ¡!!! @@@"
Output: "Tá»‘t quÃ¡"

Giá»¯ láº¡i: Chá»¯ cÃ¡i, sá»‘, khoáº£ng tráº¯ng, kÃ½ tá»± tiáº¿ng Viá»‡t
LÃ½ do: KÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cÃ³ nghÄ©a


BÆ¯á»šC 4: XÃ“A KHOáº¢NG TRáº®NG THá»ªA
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  "Sáº£n   pháº©m    tá»‘t"
Output: "Sáº£n pháº©m tá»‘t"

LÃ½ do: Chuáº©n hÃ³a format
```

---

## 9. TOKENIZATION (TÃCH Tá»ª)

### 9.1 LÃ½ thuyáº¿t Tokenization

```
TOKENIZATION = Chia vÄƒn báº£n thÃ nh cÃ¡c Ä‘Æ¡n vá»‹ nhá» (tokens)

VÃ Dá»¤ ÄÆ N GIáº¢N
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  "TÃ´i yÃªu Viá»‡t Nam"
Tokens: ["TÃ´i", "yÃªu", "Viá»‡t", "Nam"]  (4 tokens)


Váº¤N Äá»€ Vá»šI WORD-LEVEL TOKENIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Tá»« Ä‘iá»ƒn quÃ¡ lá»›n (hÃ ng triá»‡u tá»«)
2. KhÃ´ng xá»­ lÃ½ Ä‘Æ°á»£c tá»« má»›i/lá»—i chÃ­nh táº£
   â€¢ "coooool" â†’ khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn


GIáº¢I PHÃP: SUBWORD TOKENIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Chia tá»« thÃ nh cÃ¡c pháº§n nhá» hÆ¡n (subwords)

VÃ­ dá»¥:
â€¢ "unhappiness" â†’ ["un", "happiness"]
â€¢ "playing" â†’ ["play", "ing"]
```

### 9.2 BPE (Byte Pair Encoding)

```
BPE = Thuáº­t toÃ¡n tÃ¡ch tá»« Ä‘Æ°á»£c PhoBERT sá»­ dá»¥ng

NGUYÃŠN LÃ:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Báº¯t Ä‘áº§u vá»›i tá»«ng kÃ½ tá»± riÃªng láº»
2. TÃ¬m cáº·p kÃ½ tá»± xuáº¥t hiá»‡n nhiá»u nháº¥t
3. Gá»™p cáº·p Ä‘Ã³ thÃ nh 1 token má»›i
4. Láº·p láº¡i cho Ä‘áº¿n khi Ä‘áº¡t vocab size mong muá»‘n


VÃ Dá»¤ Tá»ªNG BÆ¯á»šC:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Corpus: ["low", "lowest", "newer", "wider"]

BÆ°á»›c 1: TÃ¡ch thÃ nh kÃ½ tá»±
        ["l", "o", "w", "</w>"]
        ["l", "o", "w", "e", "s", "t", "</w>"]
        ["n", "e", "w", "e", "r", "</w>"]
        ["w", "i", "d", "e", "r", "</w>"]

BÆ°á»›c 2: Äáº¿m cáº·p phá»• biáº¿n nháº¥t â†’ ("e", "r") xuáº¥t hiá»‡n 2 láº§n
        Gá»™p: "er"

BÆ°á»›c 3: Tiáº¿p tá»¥c vá»›i cáº·p tiáº¿p theo...

Káº¿t quáº£ cuá»‘i cÃ¹ng:
â€¢ "lowest" â†’ ["low", "est"]
â€¢ "newer"  â†’ ["new", "er"]


Æ¯U ÄIá»‚M Cá»¦A BPE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Tá»« Ä‘iá»ƒn nhá» gá»n (~30,000-64,000 tokens)
â€¢ Xá»­ lÃ½ Ä‘Æ°á»£c tá»« má»›i báº±ng cÃ¡ch chia nhá»
â€¢ CÃ¢n báº±ng giá»¯a word-level vÃ  character-level
```

### 9.3 Special Tokens

```
CÃC TOKEN Äáº¶C BIá»†T
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Token      â”‚ ID  â”‚ Ã nghÄ©a
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
<s>        â”‚ 0   â”‚ Báº¯t Ä‘áº§u cÃ¢u (Start)
</s>       â”‚ 2   â”‚ Káº¿t thÃºc cÃ¢u (End)
<pad>      â”‚ 1   â”‚ Äá»‡m cho cÃ¢u ngáº¯n
<unk>      â”‚ 3   â”‚ Tá»« khÃ´ng biáº¿t (Unknown)
<mask>     â”‚ -   â”‚ Tá»« bá»‹ che (cho training)


VÃ Dá»¤:
â”€â”€â”€â”€â”€â”€
Input: "Sáº£n pháº©m tá»‘t"

Tokens: ["<s>", "sáº£n", "pháº©m", "tá»‘t", "</s>"]

Token IDs: [0, 1234, 5678, 9012, 2]
```

---

## 10. EMBEDDING LAYER

### 10.1 LÃ½ thuyáº¿t Embedding

```
EMBEDDING = Ãnh xáº¡ tá»« sá»‘ sang vector

Táº¡i sao cáº§n?
â€¢ Token ID (sá»‘ nguyÃªn) khÃ´ng chá»©a ngá»¯ nghÄ©a
â€¢ Vector cÃ³ thá»ƒ biá»ƒu diá»…n quan há»‡ phá»©c táº¡p


EMBEDDING TABLE (Báº£ng tra cá»©u)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Token ID    â”‚ Vector (768 chiá»u)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0 (<s>)     â”‚ [0.01, 0.02, -0.01, ...]
1 (<pad>)   â”‚ [0.00, 0.00, 0.00, ...]
2 (</s>)    â”‚ [-0.01, 0.03, 0.02, ...]
...         â”‚ ...
1234 ("sáº£n")â”‚ [0.12, -0.34, 0.56, ...]
5678 ("tá»‘t")â”‚ [0.89, 0.23, 0.67, ...]
...         â”‚ ...

Tá»•ng: ~64,000 hÃ ng Ã— 768 cá»™t = 49 triá»‡u tham sá»‘
```

### 10.2 Ba loáº¡i Embedding trong BERT

```
CÃ”NG THá»¨C Tá»”NG Há»¢P:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E = E_token + E_position + E_segment


1. TOKEN EMBEDDING (E_token)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Tra cá»©u tá»« báº£ng embedding theo token ID
   â€¢ Má»—i tá»« â†’ 1 vector 768 chiá»u
   â€¢ Há»c Ä‘Æ°á»£c trong quÃ¡ trÃ¬nh training


2. POSITION EMBEDDING (E_position)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ MÃ£ hÃ³a vá»‹ trÃ­ cá»§a tá»« trong cÃ¢u
   â€¢ Vá»‹ trÃ­ 0, 1, 2, ... má»—i vá»‹ trÃ­ cÃ³ 1 vector riÃªng
   â€¢ PhoBERT há»— trá»£ tá»‘i Ä‘a 256 vá»‹ trÃ­

   Táº¡i sao cáº§n?
   â€¢ Transformer xá»­ lÃ½ song song â†’ khÃ´ng cÃ³ khÃ¡i niá»‡m "thá»© tá»±"
   â€¢ Position embedding cho model biáº¿t tá»« nÃ o Ä‘á»©ng trÆ°á»›c/sau

   VÃ­ dá»¥:
   â€¢ "ChÃ³ cáº¯n ngÆ°á»i" â‰  "NgÆ°á»i cáº¯n chÃ³"
   â€¢ CÃ¹ng 3 tá»« nhÆ°ng nghÄ©a khÃ¡c nhau do thá»© tá»±


3. SEGMENT EMBEDDING (E_segment)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ PhÃ¢n biá»‡t cÃ¢u A vÃ  cÃ¢u B
   â€¢ DÃ¹ng cho bÃ i toÃ¡n 2 cÃ¢u (QA, NLI)
   â€¢ Trong phÃ¢n loáº¡i cáº£m xÃºc (1 cÃ¢u): táº¥t cáº£ = 0


MINH Há»ŒA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€

Tokens: ["<s>", "sáº£n", "pháº©m", "tá»‘t", "</s>"]
Vá»‹ trÃ­:    0      1       2      3       4

E_token:   [vec0] [vec1]  [vec2] [vec3]  [vec4]
E_position:[pos0] [pos1]  [pos2] [pos3]  [pos4]
E_segment: [seg0] [seg0]  [seg0] [seg0]  [seg0]  (táº¥t cáº£ = 0)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
E_final:   [sum0] [sum1]  [sum2] [sum3]  [sum4]

Má»—i vec cÃ³ 768 chiá»u, cá»™ng element-wise
```

---

## 11. TRANSFORMER LAYER CHI TIáº¾T

### 11.1 Multi-Head Self-Attention

```
Táº I SAO "MULTI-HEAD"?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Váº¥n Ä‘á»: Má»™t attention head cÃ³ thá»ƒ bá» sÃ³t thÃ´ng tin

Giáº£i phÃ¡p: DÃ¹ng NHIá»€U heads, má»—i head há»c má»™t khÃ­a cáº¡nh

VÃ­ dá»¥ vá»›i cÃ¢u: "Con mÃ¨o Ä‘en Ä‘ang ngá»§ trÃªn gháº¿"

Head 1: Há»c quan há»‡ chá»§ - vá»‹
        "mÃ¨o" attention â†’ "ngá»§" (mÃ¨o lÃ m gÃ¬?)

Head 2: Há»c quan há»‡ bá»• nghÄ©a
        "mÃ¨o" attention â†’ "Ä‘en" (mÃ¨o nhÆ° tháº¿ nÃ o?)

Head 3: Há»c quan há»‡ vá»‹ trÃ­
        "ngá»§" attention â†’ "gháº¿" (ngá»§ á»Ÿ Ä‘Ã¢u?)

...

PhoBERT cÃ³ 12 heads â†’ há»c 12 loáº¡i quan há»‡ khÃ¡c nhau


Cáº¤U TRÃšC MULTI-HEAD:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input X (n Ã— 768)
       â”‚
       â”œâ”€â”€â†’ Head 1: Q1, K1, V1 â†’ Attention1 (n Ã— 64)
       â”œâ”€â”€â†’ Head 2: Q2, K2, V2 â†’ Attention2 (n Ã— 64)
       â”œâ”€â”€â†’ Head 3: Q3, K3, V3 â†’ Attention3 (n Ã— 64)
       â”‚    ...
       â””â”€â”€â†’ Head 12: Q12, K12, V12 â†’ Attention12 (n Ã— 64)
                           â”‚
                           â–¼
                 Concat(head1,...,head12)
                        (n Ã— 768)
                           â”‚
                           â–¼
                  Linear(W_O): 768 â†’ 768
                           â”‚
                           â–¼
                    Output (n Ã— 768)


CÃ”NG THá»¨C:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Ã— W_O

Trong Ä‘Ã³:
â€¢ head_i = Attention(Q_i, K_i, V_i)
â€¢ Q_i = Q Ã— W_Q^i (768 â†’ 64)
â€¢ K_i = K Ã— W_K^i (768 â†’ 64)
â€¢ V_i = V Ã— W_V^i (768 â†’ 64)
â€¢ W_O = Ma tráº­n output (768 Ã— 768)
```

### 11.2 Feed-Forward Network

```
SAU ATTENTION, Má»–I TOKEN ÄI QUA FFN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Cáº¥u trÃºc:
â€¢ Linear 1: 768 â†’ 3072 (má»Ÿ rá»™ng 4 láº§n)
â€¢ GELU activation
â€¢ Linear 2: 3072 â†’ 768 (thu nhá» vá»)


CÃ”NG THá»¨C:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FFN(x) = Linear2(GELU(Linear1(x)))
       = (GELU(x Ã— W1 + b1)) Ã— W2 + b2

Trong Ä‘Ã³:
â€¢ x  = Vector input (768 chiá»u)
â€¢ W1 = Trá»ng sá»‘ lá»›p 1 (768 Ã— 3072)
â€¢ b1 = Bias lá»›p 1 (3072)
â€¢ W2 = Trá»ng sá»‘ lá»›p 2 (3072 Ã— 768)
â€¢ b2 = Bias lá»›p 2 (768)


Táº I SAO Cáº¦N FFN?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Attention chá»‰ tÃ­nh tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c tá»«
â€¢ FFN thÃªm kháº£ nÄƒng há»c quan há»‡ PHI TUYáº¾N
â€¢ GiÃºp model biá»ƒu diá»…n cÃ¡c pattern phá»©c táº¡p hÆ¡n
```

### 11.3 Residual Connection vÃ  Layer Normalization

```
RESIDUAL CONNECTION (Skip Connection)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Ã tÆ°á»Ÿng: Cá»™ng input vÃ o output

y = x + F(x)

Trong Ä‘Ã³:
â€¢ x = Input ban Ä‘áº§u
â€¢ F(x) = Output tá»« sublayer (Attention hoáº·c FFN)
â€¢ y = Output cuá»‘i cÃ¹ng


Táº¡i sao cáº§n?
â€¢ Gradient cháº£y trá»±c tiáº¿p qua Ä‘Æ°á»ng táº¯t
â€¢ TrÃ¡nh vanishing gradient trong máº¡ng sÃ¢u
â€¢ Model cÃ³ thá»ƒ "bá» qua" sublayer náº¿u cáº§n (F(x) â‰ˆ 0)


LAYER NORMALIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CÃ´ng thá»©c:
LayerNorm(x) = gamma Ã— ((x - mean) / sqrt(var + eps)) + beta

Trong Ä‘Ã³:
â€¢ x = Vector input (768 chiá»u)
â€¢ mean = Trung bÃ¬nh cá»§a 768 pháº§n tá»­
â€¢ var = PhÆ°Æ¡ng sai cá»§a 768 pháº§n tá»­
â€¢ eps = 1e-6 (trÃ¡nh chia 0)
â€¢ gamma, beta = Tham sá»‘ há»c Ä‘Æ°á»£c


Táº¡i sao cáº§n?
â€¢ Chuáº©n hÃ³a giá»¯ activation trong khoáº£ng há»£p lÃ½
â€¢ Training á»•n Ä‘á»‹nh vÃ  nhanh hÆ¡n


THá»¨ Tá»° TRONG PHOBERT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â–¼                  â”‚
Attention/FFN        â”‚ (residual)
  â”‚                  â”‚
  â–¼                  â”‚
  + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
LayerNorm
  â”‚
  â–¼
Output
```

---

## 12. CLASSIFICATION VÃ€ OUTPUT

### 12.1 Pooler Layer

```
SAU 12 TRANSFORMER LAYERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Output: Ma tráº­n (n Ã— 768) vá»›i n = sá»‘ tokens

Tokens: [<s>, "sáº£n", "pháº©m", "tá»‘t", </s>]
            â”‚
            â–¼
         [h_0]  â† Vector cá»§a token <s>, 768 chiá»u

Token <s> (CLS) Ä‘Æ°á»£c dÃ¹ng lÃ m Ä‘áº¡i diá»‡n cho Cáº¢ CÃ‚U

Táº¡i sao?
â€¢ Vá»‹ trÃ­ Ä‘áº§u tiÃªn, attention Ä‘áº¿n táº¥t cáº£ tá»« khÃ¡c
â€¢ ÄÆ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin toÃ n cÃ¢u


POOLER LAYER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
h_pooled = tanh(h_0 Ã— W_pooler + b_pooler)

Trong Ä‘Ã³:
â€¢ h_0 = Vector cá»§a token <s> (768 chiá»u)
â€¢ W_pooler = Trá»ng sá»‘ (768 Ã— 768)
â€¢ b_pooler = Bias (768)
â€¢ tanh = HÃ m kÃ­ch hoáº¡t, output trong [-1, 1]
```

### 12.2 Classification Head

```
CLASSIFICATION LAYER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logits = h_pooled Ã— W_classifier + b_classifier

Trong Ä‘Ã³:
â€¢ h_pooled = Vector Ä‘áº¡i diá»‡n cÃ¢u (768 chiá»u)
â€¢ W_classifier = Trá»ng sá»‘ (768 Ã— num_classes)
â€¢ b_classifier = Bias (num_classes)
â€¢ num_classes = 3 (positive, neutral, negative)


Káº¿t quáº£:
â€¢ logits = [z_pos, z_neu, z_neg] (3 sá»‘ thá»±c)
â€¢ ChÆ°a pháº£i xÃ¡c suáº¥t (cÃ³ thá»ƒ Ã¢m, tá»•ng â‰  1)
```

### 12.3 Softmax vÃ  Cross-Entropy Loss

```
SOFTMAX
â”€â”€â”€â”€â”€â”€â”€

Chuyá»ƒn logits thÃ nh xÃ¡c suáº¥t:

P(class = k) = exp(z_k) / sum(exp(z_j))

VÃ­ dá»¥:
â€¢ logits = [2.5, 0.3, -1.2]
â€¢ exp(logits) = [12.18, 1.35, 0.30]
â€¢ sum = 13.83
â€¢ probs = [0.88, 0.10, 0.02]


CROSS-ENTROPY LOSS (Khi training)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Loss = -log(P(correct_class))

VÃ­ dá»¥:
â€¢ Label thá»±c: positive (class 0)
â€¢ P(positive) = 0.88
â€¢ Loss = -log(0.88) = 0.13

Ã nghÄ©a:
â€¢ Loss nhá» khi P(correct_class) cao
â€¢ Loss lá»›n khi P(correct_class) tháº¥p
â€¢ Training tá»‘i thiá»ƒu hÃ³a Loss â†’ tá»‘i Ä‘a hÃ³a P(correct_class)
```

---

## 13. Káº¾T QUáº¢ VÃ€ ÄÃNH GIÃ

### 13.1 Metrics Ä‘Ã¡nh giÃ¡

```
ACCURACY (Äá»™ chÃ­nh xÃ¡c)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Accuracy = Sá»‘ dá»± Ä‘oÃ¡n Ä‘Ãºng / Tá»•ng sá»‘ máº«u

VÃ­ dá»¥:
â€¢ 100 cÃ¢u test
â€¢ Model Ä‘oÃ¡n Ä‘Ãºng 85 cÃ¢u
â€¢ Accuracy = 85%


PRECISION, RECALL, F1-SCORE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Vá»›i má»—i class (vÃ­ dá»¥: positive):

                              True Positive
Precision = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            True Positive + False Positive

            (Trong cÃ¡c cÃ¢u model Ä‘oÃ¡n positive, bao nhiÃªu Ä‘Ãºng?)


                           True Positive
Recall = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         True Positive + False Negative

         (Trong cÃ¡c cÃ¢u thá»±c sá»± positive, model tÃ¬m Ä‘Æ°á»£c bao nhiÃªu?)


              2 Ã— Precision Ã— Recall
F1-Score = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            Precision + Recall

           (Trung bÃ¬nh Ä‘iá»u hÃ²a cá»§a Precision vÃ  Recall)
```

### 13.2 Confusion Matrix

```
MA TRáº¬N NHáº¦M LáºªN
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                    Predicted
                 POS   NEU   NEG
              â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    POS       â”‚  85  â”‚  10  â”‚   5  â”‚
Actual  NEU   â”‚   8  â”‚  82  â”‚  10  â”‚
    NEG       â”‚   5  â”‚  12  â”‚  83  â”‚
              â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

ÄÆ°á»ng chÃ©o = Dá»± Ä‘oÃ¡n Ä‘Ãºng
NgoÃ i Ä‘Æ°á»ng chÃ©o = Dá»± Ä‘oÃ¡n sai

VÃ­ dá»¥: Model nháº§m 10 cÃ¢u Positive thÃ nh Neutral
```

---

## 14. TÃ“M Táº®T TOÃ€N Bá»˜ CÃ”NG THá»¨C

### Báº£ng cÃ´ng thá»©c

| # | TÃªn | CÃ´ng thá»©c | Ghi chÃº |
|:-:|:----|:----------|:--------|
| 1 | Neuron | y = f(Î£w_iÃ—x_i + b) | ÄÆ¡n vá»‹ cÆ¡ báº£n |
| 2 | Sigmoid | f(x) = 1/(1+e^(-x)) | Output 0-1 |
| 3 | ReLU | f(x) = max(0, x) | Output â‰¥ 0 |
| 4 | GELU | f(x) â‰ˆ 0.5x(1+tanh(0.8x+0.04xÂ³)) | DÃ¹ng trong BERT |
| 5 | Softmax | P(k) = e^(z_k) / Î£e^(z_j) | XÃ¡c suáº¥t |
| 6 | Embedding | E = E_token + E_pos + E_seg | NhÃºng tá»« |
| 7 | Attention | Att = softmax(QK^T/âˆšd) Ã— V | CÆ¡ cháº¿ chÃº Ã½ |
| 8 | Multi-Head | MH = Concat(heads) Ã— W_O | 12 heads |
| 9 | FFN | FFN = GELU(xW1+b1)W2+b2 | Phi tuyáº¿n |
| 10 | LayerNorm | LN = Î³(x-Î¼)/Ïƒ + Î² | Chuáº©n hÃ³a |
| 11 | Residual | y = x + F(x) | Káº¿t ná»‘i táº¯t |
| 12 | Classification | logits = h Ã— W + b | PhÃ¢n loáº¡i |
| 13 | Cross-Entropy | L = -log(P_correct) | Loss |

---

## ğŸ”— THAM KHáº¢O

- [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762) - Paper gá»‘c Transformer
- [BERT: Pre-training of Deep Bidirectional Transformers (2018)](https://arxiv.org/abs/1810.04805) - Paper BERT
- [PhoBERT: Pre-trained language models for Vietnamese (2020)](https://arxiv.org/abs/2003.00744) - Paper PhoBERT
- Code: [sentiment_analyzer.py](file:///d:/LVTN/LVTN2025/sentiment-analysis-system/backend/src/services/sentiment_analyzer.py)
